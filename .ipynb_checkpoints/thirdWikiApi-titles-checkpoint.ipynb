{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawled_list = []\n",
    "created_headers = {}\n",
    "dict_col_names = {'Pos':'Position', 'Teamvte':'Team', 'Pld':'Games_Played', 'W':'Games_Won', 'D': 'Games_Drew', 'L':'Games_Lost',\\\n",
    "                  'GF':'Goals_For','GA':'Goals_Against','GD':'Goal_Difference','Pts':'Total_Points'}\n",
    "\n",
    "dict_table_names = {\n",
    "    'Team,Manager,Captain,Kit_manufacturer,Shirt_sponsor':'Kits',\n",
    "    'Rank,Player,Club,Goals':'Top_Scorers',\n",
    "    'Player,For,Against,Result,Date':'Hattricks',\n",
    "    'Position,Team,Games_Played,Games_Won,Games_Drew,Games_Lost,Goals_For,Goals_Against,Goal_Difference,Total_Points,Qualification_or_relegation':'League_Table',\n",
    "    'Team,Outgoing_manager,Manner_of_departure,Date_of_vacancy,Position_in_table,Incoming_manager,Date_of_appointment':'Managerial_Changes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trimStrBy1 (str):\n",
    "    return str[:len(str)-1]\n",
    "\n",
    "def getFullName (str):\n",
    "    str = str.strip().replace(\"\\n\",\"\").replace(\" \",\"_\")\n",
    "    #\n",
    "    if \"[\" in str:\n",
    "        str = str[:str.find(\"[\")]\n",
    "    if str not in dict_col_names:\n",
    "        return str\n",
    "    else:\n",
    "        return dict_col_names[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "from lxml.html import fromstring \n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def getTableFrame(table):\n",
    "    tmp = table.find_all('tr')\n",
    "    first = tmp[0]\n",
    "    allRows = tmp[1:]\n",
    "    headers = [header.get_text() for header in first.find_all('th')]\n",
    "    \n",
    "    # replace original headers with formatted ones\n",
    "    # also make the header string using the new formatted headers\n",
    "    lst_new_headers = []\n",
    "    new_header = ''\n",
    "    str_all_headers = ''\n",
    "    for header in headers:\n",
    "        new_header = getFullName(header)\n",
    "        lst_new_headers.append(new_header)\n",
    "        str_all_headers = str_all_headers + new_header + \",\"\n",
    "    str_all_headers = trimStrBy1(str_all_headers)\n",
    "    \n",
    "    results = [[data.get_text() for data in row.findChildren(['th', 'td'])] for row in allRows]\n",
    "    rowspan = []\n",
    "    for row_no, tr in enumerate(allRows):\n",
    "        tmp = []\n",
    "        for col_no, data in enumerate(tr.findChildren(['th', 'td'])):\n",
    "            if data.has_attr(\"rowspan\"):\n",
    "                row_span_text = data[\"rowspan\"]\n",
    "                if \"{{{rows}}}\" in row_span_text:\n",
    "                    row_span_num = 1\n",
    "                    #print (\"|||\",row_span_text,\"|||\")\n",
    "                else:\n",
    "                    row_span_num = int(data[\"rowspan\"])\n",
    "                rowspan.append((row_no, col_no, row_span_num, data.get_text()))\n",
    "    if rowspan:\n",
    "        for i in rowspan:\n",
    "            for j in range(1, i[2]):\n",
    "                results[i[0]+j].insert(i[1], i[3])\n",
    "    try:\n",
    "        df = pd.DataFrame(data=results, columns=lst_new_headers)\n",
    "        return df, str_all_headers\n",
    "    except:\n",
    "        print (\"   \",lst_new_headers)\n",
    "        print (\"   \",results)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insertIntoDB(df, table_name):\n",
    "    conn = sqlite3.connect('top5tables.sqlite')\n",
    "    cur = conn.cursor()\n",
    "    if all_headers in dict_table_names:\n",
    "        df.to_sql(con=conn, name=table_name, if_exists='append', flavor='sqlite')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting page: 1992-93_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1993-94_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1994-95_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1995-96_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1996-97_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1997-98_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1998-99_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 1999-2000_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2000-01_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2001-02_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2002-03_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2003-04_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2004-05_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2005-06_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2006–07_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2007-08_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2008-09_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2009-10_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2010-11_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2011-12_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2012-13_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2013-14_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2014-15_FA_Premier_League\n",
      "     League_Table\n",
      "   Completed\n",
      "\n",
      "Getting page: 2015–16 Premier League\n",
      "     League_Table\n",
      "   Completed\n",
      "Requested: 24\n",
      "Fetched: 24\n",
      "Success: 24\n",
      "Duplicate: 0\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from wikipedia import WikipediaPage\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "all_tables = []\n",
    "single_table = []\n",
    "single_row = []\n",
    "\n",
    "titles = ['1992-93_FA_Premier_League', \n",
    "'1993-94_FA_Premier_League', \n",
    "'1994-95_FA_Premier_League', \n",
    "'1995-96_FA_Premier_League', \n",
    "'1996-97_FA_Premier_League', \n",
    "'1997-98_FA_Premier_League', \n",
    "'1998-99_FA_Premier_League', \n",
    "'1999-2000_FA_Premier_League', \n",
    "'2000-01_FA_Premier_League', \n",
    "'2001-02_FA_Premier_League', \n",
    "'2002-03_FA_Premier_League', \n",
    "'2003-04_FA_Premier_League', \n",
    "'2004-05_FA_Premier_League', \n",
    "'2005-06_FA_Premier_League', \n",
    "'2006–07_FA_Premier_League', \n",
    "'2007-08_FA_Premier_League', \n",
    "'2008-09_FA_Premier_League', \n",
    "'2009-10_FA_Premier_League', \n",
    "'2010-11_FA_Premier_League', \n",
    "'2011-12_FA_Premier_League', \n",
    "'2012-13_FA_Premier_League', \n",
    "'2013-14_FA_Premier_League', \n",
    "'2014-15_FA_Premier_League', \n",
    "'2015–16 Premier League']\n",
    "\n",
    "request_count = 0\n",
    "fetch_count = 0\n",
    "duplicate_count = 0\n",
    "success_count = 0\n",
    "\n",
    "for title in titles:\n",
    "    try:\n",
    "        request_count += 1\n",
    "        print (\"\\nGetting page:\",title)\n",
    "        parsed_html = BeautifulSoup(wikipedia.page(title=title, redirect=True).html(), \"lxml\")\n",
    "        page_title = wikipedia.page(title).title\n",
    "        if page_title not in crawled_list:\n",
    "            fetch_count += 1\n",
    "            tables = parsed_html.findAll('table', attrs={'class':'wikitable'})\n",
    "            table_count = 0\n",
    "            for table in tables:\n",
    "                if len([header.get_text() for header in table.find_all('tr')[0].find_all('th')]) > 0:\n",
    "                    df, all_headers = getTableFrame(table)\n",
    "                    if all_headers in dict_table_names and dict_table_names[all_headers] == 'League_Table':\n",
    "                        print (\"    \",dict_table_names[all_headers])\n",
    "                        insertIntoDB(df, dict_table_names[all_headers])\n",
    "                        break\n",
    "                    table_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            print (\"   Completed\")\n",
    "            success_count += 1\n",
    "            crawled_list.append(page_title)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "            #print (\"    \",page_title)\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print (\"    Page Error\")\n",
    "    #print (\" \", page_title)\n",
    "    #except:\n",
    "        #print (\"    General Exception\")\n",
    "print (\"Requested:\", request_count)\n",
    "print (\"Fetched:\", fetch_count)\n",
    "print (\"Success:\", success_count)\n",
    "print (\"Duplicate:\", duplicate_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
